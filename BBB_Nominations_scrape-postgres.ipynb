{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e809518a-5aec-4f7c-b2e7-32b740591919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "from io import StringIO\n",
    "import csv\n",
    "import gzip\n",
    "from unidecode import unidecode\n",
    "import html5lib\n",
    "import itertools\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e027f162-505e-4588-a811-0c32fd168bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nominations_scrape(url):\n",
    "\n",
    "    # Fetch the url content\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the url content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html5lib')\n",
    "    def remove_accents(text):\n",
    "        return unidecode(text) if isinstance(text, str) else text\n",
    "\n",
    "    # Iterate through all text elements in the HTML and replace accents\n",
    "    for element in soup.find_all(string=True):\n",
    "        element.replace_with(remove_accents(element))  # Replace text with de\n",
    "\n",
    "    # Find the h2 with id=\"Histórico\"\n",
    "    h2_header = soup.find('h2', {'id': 'Histórico'})\n",
    "    desired_table = None\n",
    "\n",
    "    if h2_header:\n",
    "        parent_div = h2_header.find_parent('div')\n",
    "        next_div = parent_div.find_next_sibling()\n",
    "\n",
    "    if next_div:\n",
    "        if next_div.name == \"table\":\n",
    "            desired_table = next_div\n",
    "        else:\n",
    "            desired_table = next_div.find(\"table\", recursive=False)\n",
    "    \n",
    "    # Normalising headers\n",
    "    if desired_table:\n",
    "\n",
    "        #Parsing html table to DataFrame\n",
    "        html_to_table = pd.read_html(StringIO(str(desired_table)))\n",
    "        Nominations_raw = html_to_table[0]\n",
    "\n",
    "        # Dynamically extract all column header levels into separate lists\n",
    "        all_levels = [Nominations_raw.columns.get_level_values(level).tolist() for level in range(Nominations_raw.columns.nlevels)]\n",
    "\n",
    "        # Merging these lists into one, creating column names that contain the information from the following levels, if any\n",
    "        headers = []\n",
    "\n",
    "        for items in zip(*all_levels): # Creates tuples for each column with the different levels.\n",
    "            merged_items = []\n",
    "            for i in range(len(items)): # For each item in each tuple\n",
    "                if i == 0 or items[i] != items[i - 1]: # If it's the first item or if the item is different from the previous item.\n",
    "                    merged_items.append(items[i]) # Include the item\n",
    "                else:\n",
    "                    merged_items.append(\"\")  # Otherwise, add an empty string for duplicates\n",
    "            headers.append(\" - \".join(filter(None, merged_items)))  # Combine non-empty values\n",
    "\n",
    "        #Set new headers\n",
    "        headers = [\"\" if \"Unnamed\" in item else item for item in headers] # Removing the \"Unnamed\" levels in the header (column 0)\n",
    "        Nominations_raw.columns = headers\n",
    "\n",
    "    # Separate the 3 tables.\n",
    "\n",
    "    # Replace empty cells with nulls\n",
    "    Nominations_raw.replace(r'^\\s*$', None, regex=True, inplace=True)\n",
    "    Nominations_raw.replace(r'(nenhum)', None, regex=True, inplace=True)\n",
    "\n",
    "    # Identify rows where all columns are null - the dividers\n",
    "    divider_index = Nominations_raw[Nominations_raw.isnull().all(axis=1)].index\n",
    "\n",
    "    # Split the DataFrame into three parts based on the dividing row indices and add the headers created above\n",
    "    df_part1 = Nominations_raw.iloc[:divider_index[0]]  # From start to first blank row\n",
    "    df_part1.columns = headers\n",
    "    df_part2 = Nominations_raw.iloc[divider_index[0]+1:divider_index[1]]  # Between the blank rows\n",
    "    df_part2.columns = headers\n",
    "    df_part3 = Nominations_raw.iloc[divider_index[1]+1:]\n",
    "    df_part3.columns = headers\n",
    "\n",
    "    # Storing the individual tables into dataframes\n",
    "    Nominations = pd.DataFrame(df_part1)\n",
    "    Individual_nominations = pd.DataFrame(df_part2)\n",
    "    Eviction_results = pd.DataFrame(df_part3)\n",
    "\n",
    "    # Manipulating the Nominations table\n",
    "\n",
    "    # Remove Na columns\n",
    "    Nominations = Nominations.dropna(axis=1, how='all')\n",
    "\n",
    "    # Drop any duplicate columns to the index\n",
    "    Nominations = Nominations.loc[:, ~Nominations.T.duplicated()]\n",
    "\n",
    "    #Transpose the table\n",
    "    Nominations = Nominations.T\n",
    "    Nominations.columns = Nominations.iloc[0]\n",
    "    Nominations = Nominations[1:]\n",
    "\n",
    "    # Adding the year of the current file\n",
    "    Nominations['Edicao'] = url.rsplit('_', 1)[-1]\n",
    "\n",
    "    # Reset the index to make 'Semana' a column\n",
    "    Nominations = Nominations.reset_index()\n",
    "\n",
    "    # Create a new column 'Dinamica' based on what's after the delimiter\n",
    "    Nominations['Dinamica'] = Nominations['index'].apply(lambda x: x.split(' - ')[1] if ' - ' in x else None)\n",
    "\n",
    "    # Update 'Semana' to keep only the part before the delimiter\n",
    "    Nominations['index'] = Nominations['index'].apply(lambda x: x.split(' - ')[0] if ' - ' in x else x)\n",
    "    Nominations.rename(columns={'index': 'Semana'}, inplace=True)\n",
    "\n",
    "    # Removing spaces from column names\n",
    "    Nominations.columns = [col.replace(' ', '_') for col in Nominations.columns]\n",
    "    Nominations.columns = [col.replace('(', '') for col in Nominations.columns]\n",
    "    Nominations.columns = [col.replace(')', '') for col in Nominations.columns]\n",
    "\n",
    "\n",
    "    # Manipulating the Individual_nominations table\n",
    "\n",
    "    # Remove Na columns\n",
    "    Individual_nominations = Individual_nominations.dropna(axis=1, how='all')\n",
    "\n",
    "    #Transpose the table\n",
    "    Individual_nominations = Individual_nominations.T\n",
    "    Individual_nominations.columns = Individual_nominations.iloc[0]\n",
    "    Individual_nominations = Individual_nominations[1:]\n",
    "\n",
    "    # Drop any duplicate columns\n",
    "    Individual_nominations = Individual_nominations.loc[:, ~Individual_nominations.columns.duplicated()]\n",
    "\n",
    "    # Adding the year of the current file\n",
    "    Individual_nominations['Edicao'] = url.rsplit('_', 1)[-1]\n",
    "\n",
    "    # Reset the index to make 'Semana' a column\n",
    "    Individual_nominations = Individual_nominations.reset_index()\n",
    "\n",
    "    # Create a new column 'Dinamica' based on what's after the delimiter\n",
    "    Individual_nominations['Dinamica'] = Individual_nominations['index'].apply(lambda x: x.split(' - ')[1] if ' - ' in x else None)\n",
    "\n",
    "    # Update 'Semana' to keep only the part before the delimiter\n",
    "    Individual_nominations['index'] = Individual_nominations['index'].apply(lambda x: x.split(' - ')[0] if ' - ' in x else x)\n",
    "    Individual_nominations.rename(columns={'index': 'Semana'}, inplace=True)\n",
    "\n",
    "    # Removing spaces from column names\n",
    "    Individual_nominations.columns = [col.replace(' & ', '_') for col in Individual_nominations.columns]\n",
    "    Individual_nominations.columns = [col.replace(' ', '_') for col in Individual_nominations.columns]\n",
    "    Individual_nominations.columns = [col.replace('(', '') for col in Individual_nominations.columns]\n",
    "    Individual_nominations.columns = [col.replace(')', '') for col in Individual_nominations.columns]\n",
    "    Individual_nominations.columns = [col.replace('/', '_') for col in Individual_nominations.columns]\n",
    "    Individual_nominations.columns = [col.replace('[b]', '') for col in Individual_nominations.columns]\n",
    "\n",
    "\n",
    "    # Manipulating the Eviction_results table\n",
    "\n",
    "    # Remove Na columns\n",
    "    Eviction_results = Eviction_results.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Drop any duplicate columns to the index\n",
    "    Eviction_results = Eviction_results.loc[:, ~Eviction_results.T.duplicated()]\n",
    "\n",
    "    #Transpose the table\n",
    "    Eviction_results_t = Eviction_results.T\n",
    "    Eviction_results_t.columns = Eviction_results_t.iloc[0]\n",
    "    Eviction_results = Eviction_results_t[1:]\n",
    "\n",
    "    # Drop any duplicate columns to the index\n",
    "    Eviction_results = Eviction_results.loc[:, ~Eviction_results.T.duplicated()]\n",
    "\n",
    "    # Adding the year of the current file\n",
    "    Eviction_results['Edicao'] = url.rsplit('_', 1)[-1]\n",
    "\n",
    "    # Reset the index to make 'Semana' a column\n",
    "    Eviction_results = Eviction_results.reset_index()\n",
    "\n",
    "    # Create a new column 'Dinamica' based on what's after the delimiter\n",
    "    Eviction_results['Dinamica'] = Eviction_results['index'].apply(lambda x: x.split(' - ')[1] if ' - ' in x else None)\n",
    "\n",
    "    # Update 'Semana' to keep only the part before the delimiter\n",
    "    Eviction_results['index'] = Eviction_results['index'].apply(lambda x: x.split(' - ')[0] if ' - ' in x else x)\n",
    "    Eviction_results.rename(columns={'index': 'Semana'}, inplace=True)\n",
    "\n",
    "    # Removing spaces from column names\n",
    "    Eviction_results.columns = [col.replace(' & ', '_') for col in Eviction_results.columns]\n",
    "    Eviction_results.columns = [col.replace(' ', '_') for col in Eviction_results.columns]\n",
    "    Eviction_results.columns = [col.replace('(', '') for col in Eviction_results.columns]\n",
    "    Eviction_results.columns = [col.replace(')', '') for col in Eviction_results.columns]\n",
    "    Eviction_results.columns = [col.replace('%', 'Porcent') for col in Eviction_results.columns]\n",
    "    Eviction_results.columns = [col.replace('/', '_') for col in Eviction_results.columns]\n",
    "    Eviction_results.columns = [col.replace('[b]', '') for col in Eviction_results.columns]  \n",
    "\n",
    "    # Normalising column names\n",
    "    for col in Eviction_results.columns:\n",
    "        if col.startswith(('Outras_Porcent', 'Outras_Porcent_ou_Pontos', 'Outras_Porcent_ou_Votos','Outras_Porcent_ou_Votos')):\n",
    "            Eviction_results.rename(columns={col: 'porcent_outros_'}, inplace=True)\n",
    "\n",
    "    # Make duplicate column names unique\n",
    "    excluded = Eviction_results.columns[~Eviction_results.columns.duplicated(keep=False)]\n",
    "    counters = {}\n",
    "\n",
    "    def ren(name):\n",
    "        if name in excluded:\n",
    "            return name\n",
    "        if name not in counters:\n",
    "            counters[name] = itertools.count()  # Create a new counter\n",
    "        return f\"{name}{next(counters[name])}\"\n",
    "\n",
    "    Eviction_results.columns = [ren(name) for name in Eviction_results.columns]\n",
    "\n",
    "    # Handling duplicated values in Eliminado\n",
    "    prefixes = ('Eliminado','porcent_outros_','Expulso','Paredao')\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        columns_to_check = [col for col in Eviction_results.columns if col.startswith(prefix)]\n",
    "        if columns_to_check:\n",
    "            for index, row in Eviction_results.iterrows():\n",
    "                unique_values = list(dict.fromkeys(row[columns_to_check]))\n",
    "                for i, col in enumerate(columns_to_check):\n",
    "                    Eviction_results.loc[index, col] = unique_values[i] if i < len(unique_values) else None\n",
    "\n",
    "    Eviction_results = Eviction_results.dropna(axis=1, how='all')\n",
    "    year = url.rsplit('_', 1)[-1]\n",
    "\n",
    "    #Nominations.to_csv(f'nominations{year}')\n",
    "    #Individual_nominations.to_csv(f'individualnominations{year}')\n",
    "    #Eviction_results.to_csv(f'evictionresults{year}')\n",
    "\n",
    "    return Nominations, Individual_nominations, Eviction_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5ce48a2-5135-48fc-b7f8-9767c2d1ca3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: andrea_user\n",
      "Host: localhost\n",
      "Port: 5432\n",
      "Database: bigbrotherbrasil\n",
      "Schema: wikipedia\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "env_path = os.path.abspath(\"credentials.env\")\n",
    "\n",
    "# Load explicitly\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "# Retrieve values\n",
    "username = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "port = os.getenv('POSTGRES_PORT')\n",
    "database = os.getenv('POSTGRES_DB')\n",
    "schema = os.getenv('POSTGRES_SCHEMA')\n",
    "\n",
    "print(\"Username:\", username)\n",
    "print(\"Host:\", host)\n",
    "print(\"Port:\", port)\n",
    "print(\"Database:\", database)\n",
    "print(\"Schema:\", schema)\n",
    "\n",
    "# Build the engine\n",
    "engine = create_engine(f'postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "497cb7d8-08c6-4411-bcf1-0b8e35792509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nominations_1 written to Postgres successfully!\n",
      "individual_nominations_1 written to Postgres successfully!\n",
      "evictions_1 written to Postgres successfully!\n",
      "nominations_2 written to Postgres successfully!\n",
      "individual_nominations_2 written to Postgres successfully!\n",
      "evictions_2 written to Postgres successfully!\n",
      "nominations_3 written to Postgres successfully!\n",
      "individual_nominations_3 written to Postgres successfully!\n",
      "evictions_3 written to Postgres successfully!\n",
      "nominations_4 written to Postgres successfully!\n",
      "individual_nominations_4 written to Postgres successfully!\n",
      "evictions_4 written to Postgres successfully!\n",
      "nominations_5 written to Postgres successfully!\n",
      "individual_nominations_5 written to Postgres successfully!\n",
      "evictions_5 written to Postgres successfully!\n",
      "nominations_6 written to Postgres successfully!\n",
      "individual_nominations_6 written to Postgres successfully!\n",
      "evictions_6 written to Postgres successfully!\n",
      "nominations_7 written to Postgres successfully!\n",
      "individual_nominations_7 written to Postgres successfully!\n",
      "evictions_7 written to Postgres successfully!\n",
      "nominations_8 written to Postgres successfully!\n",
      "individual_nominations_8 written to Postgres successfully!\n",
      "evictions_8 written to Postgres successfully!\n",
      "nominations_9 written to Postgres successfully!\n",
      "individual_nominations_9 written to Postgres successfully!\n",
      "evictions_9 written to Postgres successfully!\n",
      "nominations_10 written to Postgres successfully!\n",
      "individual_nominations_10 written to Postgres successfully!\n",
      "evictions_10 written to Postgres successfully!\n",
      "nominations_11 written to Postgres successfully!\n",
      "individual_nominations_11 written to Postgres successfully!\n",
      "evictions_11 written to Postgres successfully!\n",
      "nominations_12 written to Postgres successfully!\n",
      "individual_nominations_12 written to Postgres successfully!\n",
      "evictions_12 written to Postgres successfully!\n",
      "nominations_13 written to Postgres successfully!\n",
      "individual_nominations_13 written to Postgres successfully!\n",
      "evictions_13 written to Postgres successfully!\n",
      "nominations_14 written to Postgres successfully!\n",
      "individual_nominations_14 written to Postgres successfully!\n",
      "evictions_14 written to Postgres successfully!\n",
      "nominations_15 written to Postgres successfully!\n",
      "individual_nominations_15 written to Postgres successfully!\n",
      "evictions_15 written to Postgres successfully!\n",
      "nominations_16 written to Postgres successfully!\n",
      "individual_nominations_16 written to Postgres successfully!\n",
      "evictions_16 written to Postgres successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_17748\\1437233484.py:59: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Nominations_raw.replace(r'(nenhum)', None, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nominations_17 written to Postgres successfully!\n",
      "individual_nominations_17 written to Postgres successfully!\n",
      "evictions_17 written to Postgres successfully!\n",
      "nominations_18 written to Postgres successfully!\n",
      "individual_nominations_18 written to Postgres successfully!\n",
      "evictions_18 written to Postgres successfully!\n",
      "nominations_19 written to Postgres successfully!\n",
      "individual_nominations_19 written to Postgres successfully!\n",
      "evictions_19 written to Postgres successfully!\n",
      "nominations_20 written to Postgres successfully!\n",
      "individual_nominations_20 written to Postgres successfully!\n",
      "evictions_20 written to Postgres successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_17748\\1437233484.py:59: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Nominations_raw.replace(r'(nenhum)', None, regex=True, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nominations_21 written to Postgres successfully!\n",
      "individual_nominations_21 written to Postgres successfully!\n",
      "evictions_21 written to Postgres successfully!\n",
      "nominations_22 written to Postgres successfully!\n",
      "individual_nominations_22 written to Postgres successfully!\n",
      "evictions_22 written to Postgres successfully!\n",
      "nominations_23 written to Postgres successfully!\n",
      "individual_nominations_23 written to Postgres successfully!\n",
      "evictions_23 written to Postgres successfully!\n",
      "nominations_24 written to Postgres successfully!\n",
      "individual_nominations_24 written to Postgres successfully!\n",
      "evictions_24 written to Postgres successfully!\n",
      "nominations_25 written to Postgres successfully!\n",
      "individual_nominations_25 written to Postgres successfully!\n",
      "evictions_25 written to Postgres successfully!\n"
     ]
    }
   ],
   "source": [
    "# List of URLs to process\n",
    "base_url = \"https://pt.wikipedia.org/wiki/Big_Brother_Brasil_\"\n",
    "number_of_shows = 25\n",
    "\n",
    "urls = [f\"{base_url}{i}\" for i in range(1, number_of_shows + 1)]\n",
    "\n",
    "def upload_postgres (table_name, df):\n",
    "    df.to_sql(table_name, con=engine, schema=schema, if_exists='replace', index=False)\n",
    "    print(f\"{table_name} written to Postgres successfully!\")\n",
    "\n",
    "# Iterate through URLs and process each one\n",
    "for i, url in enumerate(urls, start=1):\n",
    "    \n",
    "    # Scrape the data\n",
    "    nominations_df, individual_nominations_df, eviction_results_df = nominations_scrape(url)\n",
    "\n",
    "    # Dynamically generate table names\n",
    "    tables = {\n",
    "        f\"nominations_{i}\": nominations_df,\n",
    "        f\"individual_nominations_{i}\": individual_nominations_df,\n",
    "        f\"evictions_{i}\": eviction_results_df,\n",
    "    }\n",
    "\n",
    "    # Create tables and insert data\n",
    "    for table_name, df in tables.items():\n",
    "        upload_postgres(table_name, df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d00bd-0b87-4c5c-b837-3d4cf1412b87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
